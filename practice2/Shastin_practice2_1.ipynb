{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n, lr=0.01, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.eps = eps\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(32, 16), \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(32, self.action_n)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        return self.network(_input) \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        action_prob = self.softmax(logits).detach().numpy()\n",
    "\n",
    "        noise = np.ones(self.action_n) / self.action_n\n",
    "        action_prob_noised = (1 - self.eps) * action_prob + self.eps * noise\n",
    "        action_prob_noised = action_prob_noised / np.sum(action_prob_noised)\n",
    "        # print(f'{action_prob} -> {action_prob_noised}')\n",
    "        action = np.random.choice(self.action_n, p=action_prob_noised)\n",
    "        return action\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "\n",
    "        elite_states = np.array(elite_states)\n",
    "        elite_actions = np.array(elite_actions)\n",
    "\n",
    "        elite_states_tensor = torch.FloatTensor(elite_states)\n",
    "        elite_actions_tensor = torch.LongTensor(elite_actions)\n",
    "\n",
    "        loss = self.loss(self.forward(elite_states_tensor), elite_actions_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, visualize=False):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "    \n",
    "    for _ in range(trajectory_len):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if visualize:\n",
    "            env.render()\n",
    "            \n",
    "        trajectory['states'].append(state)\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param) \n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5)\n",
    "\n",
    "state_dim = 8\n",
    "action_n = 4\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_dim = 4\n",
    "# action_n = 2\n",
    "\n",
    "lr = 1e-2\n",
    "eps = 1\n",
    "\n",
    "agent = CEM(state_dim, action_n, lr=lr, eps=eps)\n",
    "episode_n = 100\n",
    "trajectory_n = 200\n",
    "trajectory_len = 10000\n",
    "q_param = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1, mean_total_reward = -178.51307056848768\n",
      "episode: 2, mean_total_reward = -187.0018895309032\n",
      "episode: 3, mean_total_reward = -184.9670750467903\n",
      "episode: 4, mean_total_reward = -184.6199581170169\n",
      "episode: 5, mean_total_reward = -175.29465608661428\n",
      "episode: 6, mean_total_reward = -181.06930676324163\n",
      "episode: 7, mean_total_reward = -174.50364442442267\n",
      "episode: 8, mean_total_reward = -170.61394789474093\n",
      "episode: 9, mean_total_reward = -163.40830295749782\n",
      "episode: 10, mean_total_reward = -171.87550241168978\n",
      "episode: 11, mean_total_reward = -153.38376066750382\n",
      "episode: 12, mean_total_reward = -159.67496335517194\n",
      "episode: 13, mean_total_reward = -150.7088109254332\n",
      "episode: 14, mean_total_reward = -149.6710103386742\n",
      "episode: 15, mean_total_reward = -158.75750766583292\n",
      "episode: 16, mean_total_reward = -141.18040899351206\n",
      "episode: 17, mean_total_reward = -146.59531646150225\n",
      "episode: 18, mean_total_reward = -132.35889582322295\n",
      "episode: 19, mean_total_reward = -130.1505402274524\n",
      "episode: 20, mean_total_reward = -123.33123169148199\n",
      "episode: 21, mean_total_reward = -127.17152787977204\n",
      "episode: 22, mean_total_reward = -124.04342662905277\n",
      "episode: 23, mean_total_reward = -124.56885143535784\n",
      "episode: 24, mean_total_reward = -110.84128806449573\n",
      "episode: 25, mean_total_reward = -115.55843273581934\n",
      "episode: 26, mean_total_reward = -109.98123830086584\n",
      "episode: 27, mean_total_reward = -109.42758301074474\n",
      "episode: 28, mean_total_reward = -112.65103542035934\n",
      "episode: 29, mean_total_reward = -107.79905806298557\n",
      "episode: 30, mean_total_reward = -102.54078366841023\n",
      "episode: 31, mean_total_reward = -104.54748633134041\n",
      "episode: 32, mean_total_reward = -101.97926784174958\n",
      "episode: 33, mean_total_reward = -100.43133132129847\n",
      "episode: 34, mean_total_reward = -99.53734900518236\n",
      "episode: 35, mean_total_reward = -100.60374602125059\n",
      "episode: 36, mean_total_reward = -100.59400520357299\n",
      "episode: 37, mean_total_reward = -95.06857489944106\n",
      "episode: 38, mean_total_reward = -97.05999821418683\n",
      "episode: 39, mean_total_reward = -90.78302201139755\n",
      "episode: 40, mean_total_reward = -92.2729049966261\n",
      "episode: 41, mean_total_reward = -86.5662953160701\n",
      "episode: 42, mean_total_reward = -86.39847023585371\n",
      "episode: 43, mean_total_reward = -87.22726268951857\n",
      "episode: 44, mean_total_reward = -84.15237974950324\n",
      "episode: 45, mean_total_reward = -82.74792518344111\n",
      "episode: 46, mean_total_reward = -82.5841073911337\n",
      "episode: 47, mean_total_reward = -79.62254712984601\n",
      "episode: 48, mean_total_reward = -81.53715586385742\n",
      "episode: 49, mean_total_reward = -80.66959477998127\n",
      "episode: 50, mean_total_reward = -79.20809709650467\n",
      "episode: 51, mean_total_reward = -72.2389213178339\n",
      "episode: 52, mean_total_reward = -72.45057957769312\n",
      "episode: 53, mean_total_reward = -75.74428890588779\n",
      "episode: 54, mean_total_reward = -76.27815613885303\n",
      "episode: 55, mean_total_reward = -76.63017780963291\n",
      "episode: 56, mean_total_reward = -75.75837966160458\n",
      "episode: 57, mean_total_reward = -66.6474911809715\n",
      "episode: 58, mean_total_reward = -67.13851847310322\n",
      "episode: 59, mean_total_reward = -67.82579877331439\n",
      "episode: 60, mean_total_reward = -64.50112522197725\n",
      "episode: 61, mean_total_reward = -60.052503660882415\n",
      "episode: 62, mean_total_reward = -60.58520782051563\n",
      "episode: 63, mean_total_reward = -57.2315683360151\n",
      "episode: 64, mean_total_reward = -55.490446405905274\n",
      "episode: 65, mean_total_reward = -52.68173454940192\n",
      "episode: 66, mean_total_reward = -52.115208655114856\n",
      "episode: 67, mean_total_reward = -51.19353617194458\n",
      "episode: 68, mean_total_reward = -60.46943470317456\n",
      "episode: 69, mean_total_reward = -55.487379641633524\n",
      "episode: 70, mean_total_reward = -56.8056687189959\n",
      "episode: 71, mean_total_reward = -56.64339148134406\n",
      "episode: 72, mean_total_reward = -56.31785752386457\n",
      "episode: 73, mean_total_reward = -54.649895786076016\n",
      "episode: 74, mean_total_reward = -55.91481192122796\n",
      "episode: 75, mean_total_reward = -58.684597616120556\n",
      "episode: 76, mean_total_reward = -62.09358185528361\n",
      "episode: 77, mean_total_reward = -60.48045576053507\n",
      "episode: 78, mean_total_reward = -56.632647057286064\n",
      "episode: 79, mean_total_reward = -56.01437281242592\n",
      "episode: 80, mean_total_reward = -52.12730211053159\n",
      "episode: 81, mean_total_reward = -53.517166073562436\n",
      "episode: 82, mean_total_reward = -50.85109452548849\n",
      "episode: 83, mean_total_reward = -46.88835813958114\n",
      "episode: 84, mean_total_reward = -40.10568358126952\n",
      "episode: 85, mean_total_reward = -42.77830606918429\n",
      "episode: 86, mean_total_reward = -44.678429607017044\n",
      "episode: 87, mean_total_reward = -40.81361025594289\n",
      "episode: 88, mean_total_reward = -37.26486704906322\n",
      "episode: 89, mean_total_reward = -39.00330721636919\n",
      "episode: 90, mean_total_reward = -35.26792137749333\n",
      "episode: 91, mean_total_reward = -35.41919736390716\n",
      "episode: 92, mean_total_reward = -37.436876777103336\n",
      "episode: 93, mean_total_reward = -37.8546612607719\n",
      "episode: 94, mean_total_reward = -44.66545321579379\n",
      "episode: 95, mean_total_reward = -44.77905621305872\n",
      "episode: 96, mean_total_reward = -45.01980825516584\n",
      "episode: 97, mean_total_reward = -43.840931450914866\n",
      "episode: 98, mean_total_reward = -43.71509942091539\n",
      "episode: 99, mean_total_reward = -35.6233858366763\n",
      "episode: 100, mean_total_reward = -39.095385430048346\n",
      "total time 333.49751448631287\n"
     ]
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "for episode in range(1, episode_n + 1):\n",
    "    agent.eps = np.sqrt(1 / episode)\n",
    "    # start = time.time()\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    \n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.update_policy(elite_trajectories)\n",
    "\n",
    "    # print('iteration_time: ', time.time() - start)\n",
    "# get_trajectory(env, agent, trajectory_len, visualize=True)\n",
    "print('total time', time.time() - total_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
