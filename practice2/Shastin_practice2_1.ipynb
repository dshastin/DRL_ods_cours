{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 32), \n",
    "            nn.ReLU(), \n",
    "            # nn.Linear(64, 256), \n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(32, self.action_n)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        return self.network(_input) \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        action_prob = self.softmax(logits).detach().numpy()\n",
    "        action = np.random.choice(self.action_n, p=action_prob)\n",
    "        return action\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "\n",
    "        elite_states = np.array(elite_states)\n",
    "        elite_actions = np.array(elite_actions)\n",
    "\n",
    "        elite_states_tensor = torch.FloatTensor(elite_states)\n",
    "        elite_actions_tensor = torch.LongTensor(elite_actions)\n",
    "\n",
    "        loss = self.loss(self.forward(elite_states_tensor), elite_actions_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, visualize=False):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(trajectory_len):\n",
    "        trajectory['states'].append(state)\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "                        \n",
    "    return trajectory\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param) \n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5)\n",
    "state_dim = 8\n",
    "action_n = 4\n",
    "lr = 1e-3\n",
    "\n",
    "agent = CEM(state_dim, action_n, lr=lr)\n",
    "episode_n = 50\n",
    "trajectory_n = 100\n",
    "trajectory_len = 500\n",
    "q_param = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = -199.46239613514362\n",
      "iteration_time:  1.3514821529388428\n",
      "episode: 1, mean_total_reward = -182.3854446109345\n",
      "iteration_time:  1.4778614044189453\n",
      "episode: 2, mean_total_reward = -176.7423894763417\n",
      "iteration_time:  1.4938488006591797\n",
      "episode: 3, mean_total_reward = -193.23823815642055\n",
      "iteration_time:  1.3186006546020508\n",
      "episode: 4, mean_total_reward = -167.40331441777212\n",
      "iteration_time:  1.3194832801818848\n",
      "episode: 5, mean_total_reward = -147.8692271190923\n",
      "iteration_time:  1.32137131690979\n",
      "episode: 6, mean_total_reward = -174.37895069458563\n",
      "iteration_time:  1.3932733535766602\n",
      "episode: 7, mean_total_reward = -177.9370494301973\n",
      "iteration_time:  1.3604106903076172\n",
      "episode: 8, mean_total_reward = -180.61101792144578\n",
      "iteration_time:  1.3639898300170898\n",
      "episode: 9, mean_total_reward = -161.11271475774768\n",
      "iteration_time:  1.3589904308319092\n",
      "episode: 10, mean_total_reward = -163.9249658056035\n",
      "iteration_time:  1.3242990970611572\n",
      "episode: 11, mean_total_reward = -164.0707536026499\n",
      "iteration_time:  1.3713817596435547\n",
      "episode: 12, mean_total_reward = -135.97810179351237\n",
      "iteration_time:  1.3234729766845703\n",
      "episode: 13, mean_total_reward = -139.533648679215\n",
      "iteration_time:  1.3981380462646484\n",
      "episode: 14, mean_total_reward = -144.1719740988168\n",
      "iteration_time:  1.3908312320709229\n",
      "episode: 15, mean_total_reward = -144.79601600304557\n",
      "iteration_time:  1.5174682140350342\n",
      "episode: 16, mean_total_reward = -130.92788211495258\n",
      "iteration_time:  1.4727935791015625\n",
      "episode: 17, mean_total_reward = -130.0381835288326\n",
      "iteration_time:  1.3600172996520996\n",
      "episode: 18, mean_total_reward = -153.43488836895776\n",
      "iteration_time:  1.393998622894287\n",
      "episode: 19, mean_total_reward = -150.13485608378267\n",
      "iteration_time:  1.392786979675293\n",
      "episode: 20, mean_total_reward = -129.64198212343953\n",
      "iteration_time:  1.587860107421875\n",
      "episode: 21, mean_total_reward = -160.6728621805241\n",
      "iteration_time:  1.4897050857543945\n",
      "episode: 22, mean_total_reward = -153.317260125443\n",
      "iteration_time:  1.4814691543579102\n",
      "episode: 23, mean_total_reward = -150.1585971123638\n",
      "iteration_time:  1.6040635108947754\n",
      "episode: 24, mean_total_reward = -133.10804905418794\n",
      "iteration_time:  1.520763874053955\n",
      "episode: 25, mean_total_reward = -151.52139878112163\n",
      "iteration_time:  1.4881346225738525\n",
      "episode: 26, mean_total_reward = -142.49052785929126\n",
      "iteration_time:  1.6454997062683105\n",
      "episode: 27, mean_total_reward = -137.30096143971042\n",
      "iteration_time:  1.6176555156707764\n",
      "episode: 28, mean_total_reward = -126.18769454802518\n",
      "iteration_time:  1.4875938892364502\n",
      "episode: 29, mean_total_reward = -119.3757425247669\n",
      "iteration_time:  1.6269619464874268\n",
      "episode: 30, mean_total_reward = -115.44531964591113\n",
      "iteration_time:  1.469191312789917\n",
      "episode: 31, mean_total_reward = -135.7850635533824\n",
      "iteration_time:  1.5708808898925781\n",
      "episode: 32, mean_total_reward = -124.87973758218827\n",
      "iteration_time:  1.5710811614990234\n",
      "episode: 33, mean_total_reward = -125.05826183708776\n",
      "iteration_time:  1.6737251281738281\n",
      "episode: 34, mean_total_reward = -121.86707763003922\n",
      "iteration_time:  1.6170687675476074\n",
      "episode: 35, mean_total_reward = -130.13024235970826\n",
      "iteration_time:  1.638392448425293\n",
      "episode: 36, mean_total_reward = -107.19287337449192\n",
      "iteration_time:  1.4847121238708496\n",
      "episode: 37, mean_total_reward = -127.51412145645423\n",
      "iteration_time:  1.6312148571014404\n",
      "episode: 38, mean_total_reward = -130.01420212867862\n",
      "iteration_time:  1.7521305084228516\n",
      "episode: 39, mean_total_reward = -132.20634743101004\n",
      "iteration_time:  1.8171420097351074\n",
      "episode: 40, mean_total_reward = -128.7562923956748\n",
      "iteration_time:  1.6681146621704102\n",
      "episode: 41, mean_total_reward = -117.63839333625012\n",
      "iteration_time:  1.5838878154754639\n",
      "episode: 42, mean_total_reward = -111.31062396719167\n",
      "iteration_time:  1.6030187606811523\n",
      "episode: 43, mean_total_reward = -117.97450903856276\n",
      "iteration_time:  1.6220731735229492\n",
      "episode: 44, mean_total_reward = -122.62031703724419\n",
      "iteration_time:  1.8351490497589111\n",
      "episode: 45, mean_total_reward = -112.30885681659895\n",
      "iteration_time:  1.920665979385376\n",
      "episode: 46, mean_total_reward = -104.86099703099777\n",
      "iteration_time:  1.800208330154419\n",
      "episode: 47, mean_total_reward = -111.78742140410576\n",
      "iteration_time:  1.859377384185791\n",
      "episode: 48, mean_total_reward = -112.35535409940452\n",
      "iteration_time:  1.6918983459472656\n",
      "episode: 49, mean_total_reward = -118.8226453591702\n",
      "iteration_time:  1.7469062805175781\n",
      "total time 76.81104612350464\n"
     ]
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "for episode in range(episode_n):\n",
    "    start = time.time()\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    \n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.update_policy(elite_trajectories)\n",
    "    print('iteration_time: ', time.time() - start)\n",
    "# get_trajectory(env, agent, trajectory_len, visualize=True)\n",
    "print('total time', time.time() - total_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
