{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\PycharmProjects\\DRL\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_16724\\2406726340.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  elite_states = torch.FloatTensor(elite_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = 28.4\n",
      "iteration_time:  0.03804278373718262\n",
      "episode: 1, mean_total_reward = 24.55\n",
      "iteration_time:  0.028577089309692383\n",
      "episode: 2, mean_total_reward = 30.85\n",
      "iteration_time:  0.03633928298950195\n",
      "episode: 3, mean_total_reward = 30.55\n",
      "iteration_time:  0.03494381904602051\n",
      "episode: 4, mean_total_reward = 35.35\n",
      "iteration_time:  0.03905773162841797\n",
      "episode: 5, mean_total_reward = 46.0\n",
      "iteration_time:  0.05386209487915039\n",
      "episode: 6, mean_total_reward = 52.15\n",
      "iteration_time:  0.05909156799316406\n",
      "episode: 7, mean_total_reward = 45.15\n",
      "iteration_time:  0.05202150344848633\n",
      "episode: 8, mean_total_reward = 51.05\n",
      "iteration_time:  0.05845952033996582\n",
      "episode: 9, mean_total_reward = 57.3\n",
      "iteration_time:  0.06869053840637207\n",
      "episode: 10, mean_total_reward = 64.3\n",
      "iteration_time:  0.07346439361572266\n",
      "episode: 11, mean_total_reward = 66.2\n",
      "iteration_time:  0.07727622985839844\n",
      "episode: 12, mean_total_reward = 65.45\n",
      "iteration_time:  0.0738224983215332\n",
      "episode: 13, mean_total_reward = 64.8\n",
      "iteration_time:  0.07430815696716309\n",
      "episode: 14, mean_total_reward = 80.4\n",
      "iteration_time:  0.09243655204772949\n",
      "episode: 15, mean_total_reward = 75.45\n",
      "iteration_time:  0.07938790321350098\n",
      "episode: 16, mean_total_reward = 71.15\n",
      "iteration_time:  0.07922792434692383\n",
      "episode: 17, mean_total_reward = 85.4\n",
      "iteration_time:  0.09668326377868652\n",
      "episode: 18, mean_total_reward = 96.6\n",
      "iteration_time:  0.10416483879089355\n",
      "episode: 19, mean_total_reward = 121.1\n",
      "iteration_time:  0.13730788230895996\n",
      "episode: 20, mean_total_reward = 120.2\n",
      "iteration_time:  0.13357210159301758\n",
      "episode: 21, mean_total_reward = 108.3\n",
      "iteration_time:  0.12428927421569824\n",
      "episode: 22, mean_total_reward = 118.1\n",
      "iteration_time:  0.12501025199890137\n",
      "episode: 23, mean_total_reward = 112.8\n",
      "iteration_time:  0.12378525733947754\n",
      "episode: 24, mean_total_reward = 127.5\n",
      "iteration_time:  0.13910794258117676\n",
      "episode: 25, mean_total_reward = 157.45\n",
      "iteration_time:  0.17592644691467285\n",
      "episode: 26, mean_total_reward = 159.4\n",
      "iteration_time:  0.17547369003295898\n",
      "episode: 27, mean_total_reward = 197.05\n",
      "iteration_time:  0.21715497970581055\n",
      "episode: 28, mean_total_reward = 265.9\n",
      "iteration_time:  0.28284692764282227\n",
      "episode: 29, mean_total_reward = 245.9\n",
      "iteration_time:  0.2760145664215088\n",
      "episode: 30, mean_total_reward = 271.15\n",
      "iteration_time:  0.29395389556884766\n",
      "episode: 31, mean_total_reward = 252.7\n",
      "iteration_time:  0.26846981048583984\n",
      "episode: 32, mean_total_reward = 259.2\n",
      "iteration_time:  0.28108954429626465\n",
      "episode: 33, mean_total_reward = 261.15\n",
      "iteration_time:  0.2865941524505615\n",
      "episode: 34, mean_total_reward = 315.8\n",
      "iteration_time:  0.34110403060913086\n",
      "episode: 35, mean_total_reward = 262.4\n",
      "iteration_time:  0.2902340888977051\n",
      "episode: 36, mean_total_reward = 348.6\n",
      "iteration_time:  0.3840770721435547\n",
      "episode: 37, mean_total_reward = 285.4\n",
      "iteration_time:  0.30701756477355957\n",
      "episode: 38, mean_total_reward = 324.4\n",
      "iteration_time:  0.3474268913269043\n",
      "episode: 39, mean_total_reward = 310.85\n",
      "iteration_time:  0.3421597480773926\n",
      "episode: 40, mean_total_reward = 424.1\n",
      "iteration_time:  0.4590024948120117\n",
      "episode: 41, mean_total_reward = 326.05\n",
      "iteration_time:  0.34886646270751953\n",
      "episode: 42, mean_total_reward = 365.1\n",
      "iteration_time:  0.39663195610046387\n",
      "episode: 43, mean_total_reward = 276.0\n",
      "iteration_time:  0.2969386577606201\n",
      "episode: 44, mean_total_reward = 359.7\n",
      "iteration_time:  0.38436412811279297\n",
      "episode: 45, mean_total_reward = 406.4\n",
      "iteration_time:  0.433361291885376\n",
      "episode: 46, mean_total_reward = 343.5\n",
      "iteration_time:  0.3633584976196289\n",
      "episode: 47, mean_total_reward = 309.95\n",
      "iteration_time:  0.33754444122314453\n",
      "episode: 48, mean_total_reward = 358.45\n",
      "iteration_time:  0.38033032417297363\n",
      "episode: 49, mean_total_reward = 348.35\n",
      "iteration_time:  0.3726356029510498\n"
     ]
    }
   ],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 100), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(100, self.action_n)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        return self.network(_input) \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        action_prob = self.softmax(logits).detach().numpy()\n",
    "        action = np.random.choice(self.action_n, p=action_prob)\n",
    "        return action\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(elite_states)\n",
    "        elite_actions = torch.LongTensor(elite_actions)\n",
    "        \n",
    "        loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "def get_trajectory(env, agent, trajectory_len, visualize=False):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "    \n",
    "    for _ in range(trajectory_len):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if visualize:\n",
    "            env.render()\n",
    "            \n",
    "        trajectory['states'].append(state)\n",
    "            \n",
    "    return trajectory\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param) \n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "action_n = 2\n",
    "\n",
    "agent = CEM(state_dim, action_n)\n",
    "episode_n = 50\n",
    "trajectory_n = 20\n",
    "trajectory_len = 500\n",
    "q_param = 0.8\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    start = time.time()\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    \n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.update_policy(elite_trajectories)\n",
    "    print('iteration_time: ', time.time() - start)\n",
    "# get_trajectory(env, agent, trajectory_len, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
