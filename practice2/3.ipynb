{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc30feb-6cb3-4d18-9858-485a4d39f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c5df09-7341-4f1a-a93f-a717dd6ee85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "acdtion_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8089fa-6780-4e86-8753-aae3cbf45608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.action_n)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, _input):\n",
    "        return self.network(_input)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        probs = self.softmax(logits).detach().numpy()\n",
    "        action = np.random.choice(self.action_n, p=probs)\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        elite_actions = torch.LongTensor(np.array(elite_actions))\n",
    "        \n",
    "        pred_actions = self.forward(elite_states)\n",
    "        loss = self.loss(pred_actions, elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0ff4d0-d3ae-452a-acd3-8de655ab7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, max_steps, visualize=False):\n",
    "    trajectory = {'states': [], 'actions': [], 'total_reward': 0}\n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "\n",
    "    for s in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        trajectory['states'].append(state)\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240add98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param) \n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517e56cb-1d22-4b52-8101-b6e6c40752e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\PycharmProjects\\DRL\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, mean_total_reward = 23.55\n",
      "episode: 1, mean_total_reward = 29.9\n",
      "episode: 2, mean_total_reward = 29.25\n",
      "episode: 3, mean_total_reward = 38.0\n",
      "episode: 4, mean_total_reward = 40.85\n",
      "episode: 5, mean_total_reward = 35.45\n",
      "episode: 6, mean_total_reward = 42.1\n",
      "episode: 7, mean_total_reward = 45.25\n",
      "episode: 8, mean_total_reward = 44.25\n",
      "episode: 9, mean_total_reward = 42.7\n",
      "episode: 10, mean_total_reward = 52.5\n",
      "episode: 11, mean_total_reward = 49.2\n",
      "episode: 12, mean_total_reward = 59.15\n",
      "episode: 13, mean_total_reward = 53.55\n",
      "episode: 14, mean_total_reward = 65.8\n",
      "episode: 15, mean_total_reward = 80.25\n",
      "episode: 16, mean_total_reward = 65.0\n",
      "episode: 17, mean_total_reward = 74.55\n",
      "episode: 18, mean_total_reward = 87.25\n",
      "episode: 19, mean_total_reward = 107.15\n",
      "episode: 20, mean_total_reward = 89.75\n",
      "episode: 21, mean_total_reward = 107.9\n",
      "episode: 22, mean_total_reward = 109.45\n",
      "episode: 23, mean_total_reward = 101.9\n",
      "episode: 24, mean_total_reward = 133.0\n",
      "episode: 25, mean_total_reward = 147.45\n",
      "episode: 26, mean_total_reward = 142.4\n",
      "episode: 27, mean_total_reward = 195.1\n",
      "episode: 28, mean_total_reward = 225.95\n",
      "episode: 29, mean_total_reward = 275.4\n",
      "episode: 30, mean_total_reward = 213.65\n",
      "episode: 31, mean_total_reward = 254.75\n",
      "episode: 32, mean_total_reward = 239.55\n",
      "episode: 33, mean_total_reward = 264.3\n",
      "episode: 34, mean_total_reward = 218.0\n",
      "episode: 35, mean_total_reward = 249.0\n",
      "episode: 36, mean_total_reward = 221.95\n",
      "episode: 37, mean_total_reward = 251.3\n",
      "episode: 38, mean_total_reward = 284.7\n",
      "episode: 39, mean_total_reward = 332.05\n",
      "episode: 40, mean_total_reward = 339.3\n",
      "episode: 41, mean_total_reward = 301.2\n",
      "episode: 42, mean_total_reward = 318.25\n",
      "episode: 43, mean_total_reward = 383.4\n",
      "episode: 44, mean_total_reward = 396.05\n",
      "episode: 45, mean_total_reward = 357.35\n",
      "episode: 46, mean_total_reward = 362.95\n",
      "episode: 47, mean_total_reward = 380.25\n",
      "episode: 48, mean_total_reward = 387.45\n",
      "episode: 49, mean_total_reward = 314.9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "action_n = 2\n",
    "\n",
    "agent = CEM(state_dim, action_n)\n",
    "episode_n = 50\n",
    "trajectory_n = 20\n",
    "trajectory_len = 500\n",
    "q_param = 0.8\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "    \n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4580283-dec2-4b48-9557-8c7d8413ee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0597ee-7429-4077-9e18-bc2042e26de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
