{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc30feb-6cb3-4d18-9858-485a4d39f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c5df09-7341-4f1a-a93f-a717dd6ee85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "acdtion_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8089fa-6780-4e86-8753-aae3cbf45608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_n)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, _input):\n",
    "        return self.network(_input)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        probs = self.softmax(logits).data.numpy()\n",
    "        action = np.random.choice(self.action_n, p=probs)\n",
    "        return action\n",
    "\n",
    "    def fit(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, actions in zip(trajectory['states'], trajectory['actions']):\n",
    "                elite_states.append(state)\n",
    "                elite_actions.append(actions)\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        elite_actions = torch.LongTensor(np.array(elite_actions))\n",
    "        pred_actions = self.forward(elite_states)\n",
    "        loss = self.loss(pred_actions, elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0ff4d0-d3ae-452a-acd3-8de655ab7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, max_steps, visualize=False):\n",
    "    state = env.reset()\n",
    "    trajectory = {'states': [], 'actions': [], 'rewards': []}\n",
    "    for s in range(max_steps):\n",
    "        trajectory['states'].append(state)\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['actions'].append(action)\n",
    "        trajectory['rewards'].append(reward)\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc12f1e7-31c6-4340-a74b-2cb1f27c8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "max_steps = 500\n",
    "trajectory_n = 50\n",
    "q=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517e56cb-1d22-4b52-8101-b6e6c40752e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17038247\\PycharmProjects\\DRL_ods_cours\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0/100, mean_total_reward: 23.18\n",
      "fit time: 0.34783291816711426\n",
      "fit time: 0.3802661895751953\n",
      "fit time: 0.36266350746154785\n",
      "fit time: 0.43755125999450684\n",
      "fit time: 0.4734065532684326\n",
      "fit time: 0.5512897968292236\n",
      "fit time: 0.5538144111633301\n",
      "fit time: 0.6167526245117188\n",
      "fit time: 0.6684765815734863\n",
      "fit time: 0.6773273944854736\n",
      "iteration: 10/100, mean_total_reward: 59.86\n",
      "fit time: 0.7562265396118164\n",
      "fit time: 0.8239850997924805\n",
      "fit time: 0.7979583740234375\n",
      "fit time: 0.9094693660736084\n",
      "fit time: 0.8120217323303223\n",
      "fit time: 0.9461963176727295\n",
      "fit time: 0.9251337051391602\n",
      "fit time: 1.0181291103363037\n",
      "fit time: 1.2618491649627686\n",
      "fit time: 1.355663776397705\n",
      "iteration: 20/100, mean_total_reward: 101.86\n",
      "fit time: 1.2522380352020264\n",
      "fit time: 1.4808483123779297\n",
      "fit time: 1.8177599906921387\n",
      "fit time: 1.9115183353424072\n",
      "fit time: 1.8199975490570068\n",
      "fit time: 2.3346142768859863\n",
      "fit time: 2.852109909057617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = CEM(state_dim, acdtion_n)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    start = time.time()\n",
    "    trajectories = [get_trajectory(env, agent, max_steps=max_steps) for _ in range(trajectory_n)]\n",
    "    total_rewards = [np.sum(trajectory['rewards']) for trajectory in trajectories]\n",
    "    if i % 10 == 0:\n",
    "        print(f'iteration: {i}/{n_iterations}, mean_total_reward: {np.mean(total_rewards)}')\n",
    "\n",
    "    quantile = np.quantile(total_rewards, q)\n",
    "    elite_trajectories = []\n",
    "    for t in trajectories:\n",
    "        total_reward = np.sum(t['rewards'])\n",
    "        if total_reward >= quantile:\n",
    "            elite_trajectories.append(t)\n",
    "    if len(elite_trajectories) == 0:\n",
    "        print(total_rewards)\n",
    "        print(quantile)\n",
    "    agent.fit(elite_trajectories)\n",
    "    print(f'fit time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4580283-dec2-4b48-9557-8c7d8413ee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0597ee-7429-4077-9e18-bc2042e26de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
